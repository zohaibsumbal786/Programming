import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import mpld3
import os
import time

# Start measuring execution time
start_time = time.time()

# Load the dataset into a DataFrame
df = pd.read_csv(r'C:\Users\zohai\OneDrive\Desktop\youtube.csv')

# Display the column names to verify the dataset
print("Columns in the dataset:")
print(df.columns)

# Select key features
key_features = ['views', 'likes', 'dislikes', 'comment_count', 'publish_date']
df = df[key_features]

# Handle missing values by dropping rows with NaN values
df = df.dropna()

# Feature engineering: Parse the publish_date column correctly
df['publish_date'] = pd.to_datetime(df['publish_date'], dayfirst=True)
df['publish_year'] = df['publish_date'].dt.year
df['publish_month'] = df['publish_date'].dt.month
df['publish_day'] = df['publish_date'].dt.day
df.drop('publish_date', axis=1, inplace=True)

# Verify the data types
print(df.dtypes)

# Log transformation of 'views' to handle skewness
df['log_views'] = np.log1p(df['views'])

# Standardize numerical features
scaler = StandardScaler()
numerical_features = ['likes', 'dislikes', 'comment_count']
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Data splitting
X = df.drop(['views', 'log_views'], axis=1)
y = df['log_views']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to evaluate model
def evaluate_model(model, X_test, y_test):
    y_pred_log = model.predict(X_test)
    y_pred_log[y_pred_log < 0] = 0  # Ensure predictions are non-negative
    y_pred = np.expm1(y_pred_log)
    y_true = np.expm1(y_test)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, r2

# Decision Tree Regression with hyperparameter tuning
param_grid_dt = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 10, 20]}
grid_search_dt = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_dt, cv=5)
grid_search_dt.fit(X_train, y_train)
best_dt_model = grid_search_dt.best_estimator_

mse_dt, rmse_dt, r2_dt = evaluate_model(best_dt_model, X_test, y_test)
print("Decision Tree Regression Report:")
print(f"Mean Squared Error: {mse_dt}")
print(f"Root Mean Squared Error: {rmse_dt}")
print(f"R2 Score: {r2_dt}")

# KNN Regression with hyperparameter tuning
param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}
grid_search_knn = GridSearchCV(KNeighborsRegressor(), param_grid_knn, cv=5)
grid_search_knn.fit(X_train, y_train)
best_knn_model = grid_search_knn.best_estimator_

mse_knn, rmse_knn, r2_knn = evaluate_model(best_knn_model, X_test, y_test)
print("KNN Regression Report:")
print(f"Mean Squared Error: {mse_knn}")
print(f"Root Mean Squared Error: {rmse_knn}")
print(f"R2 Score: {r2_knn}")

# Simplify SVM training
print("Starting SVM training...")
svm_model = SVR(kernel='linear')
svm_model.fit(X_train, y_train)
mse_svm, rmse_svm, r2_svm = evaluate_model(svm_model, X_test, y_test)
print("SVM Regression Report:")
print(f"Mean Squared Error: {mse_svm}")
print(f"Root Mean Squared Error: {rmse_svm}")
print(f"R2 Score: {r2_svm}")

# Visualizations
sns.pairplot(df, diag_kind='kde')
pairplot_html = mpld3.fig_to_html(plt.gcf())
pairplot_path = r'C:\Users\zohai\OneDrive\Desktop\pairplot.html'
with open(pairplot_path, 'w') as f:
    f.write(pairplot_html)
print("Pairplot saved at:", pairplot_path)
plt.close()

plt.figure(figsize=(12, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
heatmap_html = mpld3.fig_to_html(plt.gcf())
heatmap_path = r'C:\Users\zohai\OneDrive\Desktop\heatmap.html'
with open(heatmap_path, 'w') as f:
    f.write(heatmap_html)
print("Heatmap saved at:", heatmap_path)
plt.close()

plt.figure(figsize=(12, 6))
sns.lineplot(data=df, x='publish_year', y='views')
plt.title('Trends in Video Views Over Time')
lineplot_html = mpld3.fig_to_html(plt.gcf())
lineplot_path = r'C:\Users\zohai\OneDrive\Desktop\lineplot.html'
with open(lineplot_path, 'w') as f:
    f.write(lineplot_html)
print("Lineplot saved at:", lineplot_path)
plt.close()

feature_importances = pd.Series(best_dt_model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(12, 6))
feature_importances.plot(kind='bar')
plt.title('Feature Importances from Decision Tree')
feature_importances_html = mpld3.fig_to_html(plt.gcf())
feature_importances_path = r'C:\Users\zohai\OneDrive\Desktop\feature_importances.html'
with open(feature_importances_path, 'w') as f:
    f.write(feature_importances_html)
print("Feature Importances plot saved at:", feature_importances_path)
plt.close()

# Stop measuring execution time
end_time = time.time()
execution_time = end_time - start_time
print(f"Total execution time: {execution_time:.2f} seconds")

# Discuss insights
print("Feature Importances from Decision Tree Regression Model:")
print(feature_importances)

# Discuss implications
print("Insights for content creators and advertisers:")
print("1. High engagement (likes, comments) correlates with higher views.")
print("2. Certain categories may have more consistent popularity.")
print("3. Video duration and publish date can impact viewer engagement.")

print("Implications for content creators:")
print("1. Focus on producing content that encourages likes and comments.")
print("2. Analyze popular categories and align content accordingly.")
print("3. Consider optimal video length for engagement.")

print("Implications for advertisers:")
print("1. Target ads in")
